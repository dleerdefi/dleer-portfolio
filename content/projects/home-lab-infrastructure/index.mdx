---
title: "Home Lab Infrastructure"
slug: "home-lab-infrastructure"
summary: "Enterprise-grade home lab with GPU cluster and automation"
date: "2024-11-01"
updated: "2025-10-16"
tags: ["homelab", "infrastructure", "kubernetes", "proxmox", "gpu-cluster"]
tech: ["Proxmox", "TrueNAS", "OPNsense", "Ansible", "Docker"]
cover: ""
status: "production"
featured: false
---

## Overview

Enterprise homelab infrastructure featuring 4 Dell PowerEdge servers running Proxmox with **1.3TB total ECC RAM**, 88-core Xeon CPUs, and **2x RTX A4000 GPUs** for local LLM inference and real-time object detection. The cluster includes **60TB NAS storage** via TrueNAS and 10GbE networking backbone.

Currently preparing for network expansion with OPNsense firewall deployment, managed switch upgrades, and enterprise wireless access points. Production environment supporting AI research, media streaming, home automation, and self-hosted services running 24/7.

<Terminal cmd="network architecture">
                 ┌─────────────┐
                 │  Internet   │
                 └──────┬──────┘
                        │
                 ┌──────▼──────┐
                 │  OPNsense   │
                 │  Firewall   │  (planned)
                 └──────┬──────┘
                        │
              ┌─────────▼─────────┐
              │  Core Switch      │
              │  + WiFi APs       │  (expansion)
              └┬──┬──┬──┬──┬──┬──┘
               │  │  │  │  │  │
        ┌──────┘  │  │  │  │  └─────┐
        │         │  │  │  │        │
    ┌───▼──┐  ┌──▼──▼──▼──▼──┐  ┌──▼──┐
    │TrueNAS│ │   Proxmox     │  │ POE │
    │ 60TB │  │ Cluster (4x)  │  │Cams │
    └──────┘  └───┬───┬───┬───┘  └─────┘
                  │   │   │
              ┌───▼┐ ┌▼──┐▼───┐
              │GPU │ │GPU│VMs │
              │LLM │ │CV │    │
              └────┘ └───┘────┘
</Terminal>

<Window title="proxmox/deploy-gpu-vm.yml">
```yaml
---
# Ansible playbook for GPU-accelerated LLM deployment
- name: Deploy LLM inference VM with GPU passthrough
  hosts: r730_proxmox
  tasks:
    - name: Clone GPU-enabled template
      proxmox_kvm:
        api_host: "192.168.0.58"
        vmid: "{{ new_vmid }}"
        clone: "gpu-template"
        name: "llm-{{ model_name }}"
        cores: 16
        memory: 65536

    - name: Configure PCIe GPU passthrough
      proxmox_kvm:
        vmid: "{{ new_vmid }}"
        hostpci0: "0000:3b:00.0,pcie=1"  # RTX A4000
        machine: "q35"

    - name: Deploy Ollama with model
      shell: |
        ssh vm-{{ new_vmid }} "
          docker run -d --gpus all \
            -v ollama:/root/.ollama \
            -p 11434:11434 \
            ollama/ollama
          docker exec ollama ollama pull {{ model_name }}
        "
```
</Window>

**Links:**
- Hardware specs and network diagrams coming soon
- Infrastructure configs: Private repository
