---
title: "Building Software Like a Builder"
slug: "building-software-like-a-builder"
summary: "Stop 'vibe coding.' Use a 'preconstruction mindset' to harness agentic AI and ship software that actually works. Templates and checklists inside."
date: "2025-10-17"
updated: "2025-11-03"
tags: ["engineering", "ai", "workflow", "spec-driven-development"]
status: "published"
series: "Preconstruction Mindset"
---

<Admonition type="info" title="The Core Insight">
**The preconstruction mindset**: plan first with ruthless clarity, then execute in slices. It's the pivot from *coder* to *project manager*, and it's the single most important skill for building with AI.
</Admonition>

## The Jobsite and the Codebase

I didn't learn to ship software in a CS lab. I learned it on muddy construction sites, helping engineer a **$548 million medical campus**. My job was to make sure the hidden systems–medical gas, reverse-osmosis lines, even pneumatic tubes–would mesh perfectly within the concrete and steel. You learn to think in dependencies and sequencing because the only alternative is chaos.

<Figure
  src="/images/dleer-tower-crane.webp"
  alt="Hanging the WSU flag on a tower crane at St. Michael Medical Center"
  caption="Hanging the WSU flag on a tower crane at St. Michael Medical Center in Silverdale, WA"
  width={1032}
  height={1032}
  size="medium"
/>

Years later, as a Chief of Product who regularly ships code using AI, I operate the same way.
I call it the **preconstruction mindset**: plan first, then execute in slices.

In the construction industry, the "preconstruction" phase is recognized as ["the backbone of any construction project"](https://www.letsbuild.com/blog/what-is-preconstruction-and-why-its-important). It's a defined discipline for ["risk mitigation, cost control, project planning, and quality assurance"](https://www.procore.com/library/preconstruction-software). It's the phase where all stakeholders align on the scope, budget, and schedule specifically to identify potential roadblocks *before* a single foundation is poured.

As I've applied construction's rigor to software, I've watched the construction industry start to apply software's flexibility to its own planning. New research highlights how construction teams are ["applying the Agile methodology in the pre-construction phase"](https://www.mdpi.com/2075-5309/14/11/3551) to "enhance flexibility and collaboration" and "adapt quickly to evolving project requirements".

This creates a powerful feedback loop. Construction's rigor provides the *discipline* AI-led software development desperately needs. Software's iteration (Agile) provides the *flexibility* complex engineering projects demand.

My "preconstruction mindset" is a synthesis of both. It's the pivot from *coder* to *project manager*, and it's the single most important skill for building with AI.

## The Siren Song of "Vibe Coding"

Generative AI makes it dangerously tempting to adopt the *wrong* workflow. Andrej Karpathy gave this a name:

<Admonition type="warning" title="Vibe Coding">
**Vibe coding:** when you kind of vaguely know what you want but not really, so you just throw some code together until it feels right.
</Admonition>

It's "artistic sketching", or as I've seen it, "prompt, paste, and hope". You'll get 800 lines of code in minutes, then spend the next 48 hours spamming "pls fix" for errors in code you didn't design and don't understand.

This is the **Illusion of Velocity**.

A McKinsey study claimed developers using generative AI can complete coding tasks up to twice as fast. But this speed is deceptive. As *The Pragmatic Engineer*'s Gergely Orosz points out, the speed of typing out code has never been the bottleneck for software development.

The *real* bottlenecks are clarifying requirements and reviewing code. Vibe coding doesn't solve these bottlenecks; it just moves them and makes them worse.

But this isn't a failure of AI; it's a failure of *method*. It's the human-in-the-loop failing to provide direction. The skills that make a good *builder*–like planning, risk assessment, and review–are now more valuable than ever.

## The Antidote: Spec-Driven Development

The antidote to vibe coding is a short, explicit spec. This isn't just a good idea; it's the *intended workflow* for the most powerful agentic tools. Anthropic's Claude Code, for example, is built on a "plan, then execute" strategy. You explicitly ask for a plan, review it, and *then* give the agent the green light to build.

My "preconstruction mindset" has a formal name for this: **Spec-Driven Development (SDD)**. It is being called the ["golden workflow of AI coding"](https://medium.com/@shenli3514/spec-driven-development-sdd-is-the-future-of-software-engineering-85b258cea241).

Birgitta Böckeler, writing for Martin Fowler's site, defines it perfectly:

<Admonition type="tip" title="Spec-Driven Development">
**Spec-driven development means writing a 'spec' before writing code with AI ('documentation first'). [The spec becomes the source of truth for the human and the AI](https://martinfowler.com/articles/exploring-gen-ai/to-vibe-or-not-vibe.html)**.
</Admonition>

This simple process tames the chaos of prompt-driven vibe coding. The spec acts as a "North Star" for the AI agent, allowing it to take on larger tasks without getting lost.

The core benefit, as the GitHub blog explains, is [**"separating the stable 'what' from the flexible 'how'"**](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/).

* The **"what"** is the *intent*: the user's problem, the business goal.
* The **"how"** is the *implementation*: the tech stack, the API contracts, the database schema.

This separation allows you to iterate *on the spec*–the "what"–and let the AI handle the "expensive rewrites" of the "how".

### Bridging the "Unknown Unknowns" (for non-coders)

This shift to "plan-then-execute" is revolutionary because it reframes the developer's job as a *project manager*. Your value is no longer in typing, but in defining the *what* and verifying the *how*.

But this presents a new challenge. A good project manager *must* have a strong understanding of what they are managing. If you don't have a technical CS background, you will face a wall of "unknown unknowns".

Here, we can use AI to build the plan *itself*. A powerful technique is to use LLMs in an "adversarial" or "critique" framework:

1. **Generate:** Ask one agent (e.g., Claude) to generate a full technical spec for your goal.
2. **Critique:** Ask a *different* agent (e.g., GPT-4o) to act as a cynical, senior-level "reviewer" and find every flaw, risk, and missing piece in that spec.

This "LLM-as-reviewer" process uses AI to surface the very "unknowns" that would otherwise sink your project.

## Phase 1: Preconstruction

The spec-driven mindset pairs best with tools built for it. You have a few options:

* **Agentic IDEs (e.g., Cursor):** Powerful, but can be a short path to vibe-coding debt if you skip the spec.
* **Terminal-first, Plan-then-Execute Agents (e.g., Claude Code):** These tools are *designed* for this workflow. They encourage you to review a plan *before* files are changed, which pairs perfectly with a written spec.

### The Spec

Copy this template into `docs/SPEC.md` in your repository. Fill it in before you write a single line of code. Treat it as the contract for what you, and your AI, are building. This is the spec I use.

<Window title="docs/SPEC.md">

# Project Spec

## 1. Goal
* **For:** Busy people who lose track of tasks
* **Problem:** Tasks scattered across devices; no fast, reliable sync
* **Success:** TTI < 2s, task ops < 150ms, p50 sync < 800ms, DAU > 100 by week 2

## 2. System Components
* **Entities:** User \{ id, email \}, Task \{ id, userId, title, status, dueAt \}, List \{ id, userId, name \}
* **Actions:** auth:signInWithGoogle, task:create|edit|complete, list:create|share
* **Boundaries:** Web (Next.js) / API (FastAPI) / DB (Postgres) / Cache (Redis)

## 3. Constraints
* **Stack:** Next.js + FastAPI + Postgres + Redis; Auth via OAuth (Google)
* **Libraries:** Auth (NextAuth), ORM (Prisma/SQLModel), Logging (Pino/Stdlib), Telemetry (OTLP)
* **Non-negotiables:** API contracts documented in openapi.yaml; migrations versioned

## 4. Acceptance Criteria (examples)
* **Complete Task:** Clicking "Complete" moves task to Done within **1s**; API returns **200**; UI optimistic-updates and reconciles on server response.
* **Sync:** Creating a task on device A shows on device B within **800ms** (p50) during a 2-device test.

## 5. Work Packages (1–2 days each)
* **M1:** Auth + User model (Google sign-in persists session; /me returns user)
* **M2:** Tasks CRUD (create/edit/complete works; unit tests for service layer)
* **M3:** Realtime sync (2-device test meets p50 < 800ms)
* **M4:** Share read-only list links (public route guarded, no edit leaks)

## 6. Risks
* **Auth vendor lock-in** → Wrap auth in an internal adapter; keep user schema portable.

## 7. Open Questions
* Roles (admin/editor) needed?
* Offline first? If yes, queue + reconcile strategy.

## 8. Repo Layout
* apps/web
* apps/api
* packages/ui
* infra/docker
* docs/SPEC.md

## 9. Environments
* **.env.example:** DATABASE_URL=..., REDIS_URL=..., GOOGLE_CLIENT_ID=...

</Window>

## Phase 2: Construction (The Loop)

With your SPEC.md in the repo, you're ready for construction. Now, it's time to follow a tight, disciplined loop.

<Terminal>
# 1. Create a branch for one slice (e.g., feature/M1-auth)
git checkout -b feature/M1-auth

# 2. Give your spec to your coding agent and ask it to build *only that slice*
# Agent reads docs/SPEC.md and implements M1

# 3. Run the acceptance checks to verify the work
npm run build && npm run test

# 4. If anything needs to change, update the spec first, then the code

# 5. Commit small and often
git add . && git commit -m "feat: implement M1 auth"
</Terminal>

This loop redefines the human's job. Your value is no longer *typing*; it's *verifying*. This is critical because, as AI experts will tell you:

<Admonition type="warning" title="Critical Understanding">
**[LLMs are NOT compilers... they are inferrers. A compiler takes a structured input and produces a repeatable, predictable output. LLMs do not do that](https://martinfowler.com/articles/exploring-gen-ai/i-still-care-about-the-code.html)**.
</Admonition>

Because AI is a non-deterministic "inferrer," you *must* perform a constant risk assessment for every line of code it produces. My loop *is* that risk assessment. It systematically manages all three risk factors:

* **Probability:** You *lower* the probability of AI error by giving it a clear, unambiguous spec.
* **Impact:** You *lower* the impact of an error by building in small, isolated "work packages."
* **Detectability:** You *maximize* your ability to detect errors by running "acceptance checks" against the spec.

## Phase 3: Post-Construction (The Punch List)

When a building is "finished," there's still work to do: final inspections, owner training, and fixing all the small issues found on the final walkthrough. We call this the "punch list."

Shipping an MVP is the same. Once real users arrive, you learn what's broken, what's slow, and what features are actually valuable. The work becomes a steady rhythm of fixing bugs, measuring performance, and turning feature requests into clear stories in your spec.

Reaching this stage is a massive win.

<Admonition type="tip">
**Having users who care enough to complain about your product is the best possible outcome for a builder.**
</Admonition>

## Appendix: Deploy from Day Zero

On the jobsite, you don't build a $500M hospital and *then* ask the city for permits. Deployment, like regulatory compliance, must be planned from day one.

If you ignore deployment until the end, your app will never graduate from localhost. Use this checklist from day zero.

<Collapsible title="Deployment Checklist">

* **A dev environment that mirrors production.** Your local setup, ideally run via Docker, must use the same base image and OS libraries as your production environment. This prevents mismatches and is the industry standard.
* **No secrets in Git.** An .env.example file is committed, but real secrets are only ever loaded from environment variables. This is a fundamental security control.
* **Consistent storage layers.** Your data stores (Postgres, Redis, etc.) run identically across all environments, managed locally with Docker Compose.
* **A stateless and scalable app.** The application is designed to be stateless. Any persistent files (like user uploads) are written to external object storage (like S3), not the container's filesystem. This is non-negotiable for modern, cloud-native hosting.
* **Health endpoints are implemented.** Your app includes /health (liveness) and /ready (readiness) endpoints. Hosting platforms like Kubernetes *require* these to manage your app and enable zero-downtime deploys.
* **An optimized, multi-stage Dockerfile.** Assets are built in an initial stage, and the final image contains only the lean, necessary runtime code.
* **Basic telemetry is included.** The application logs essential metrics, such as request duration and error rates. This enables the "ongoing monitoring" that feeds your "punch list".
* **A simple, repeatable deploy script.** You have a one-command script (e.g., fly deploy or a custom shell script) to deploy your application. This is the foundation of Continuous Delivery.

</Collapsible>

## Conclusion: The Builder's Advantage

AI *is* changing the landscape. The skills are shifting from pure technical implementation to strategic project management. As Gergely Orosz says, AI will not replace you but will make you more productive—you must constantly learn.

The preconstruction mindset *is* the workflow for learning. It's how you harness the power of AI without inheriting its chaos. It's how you stay in control.

Your unique domain expertise is your greatest advantage. The next wave of great applications will come from people outside the tech bubble who understand real-world problems. With the right workflow, you have everything you need to build one of them.

If you're a solo builder using AI, steal the spec, run the loop, and deploy from day one.

---

## Frequently Asked Questions

<Collapsible title="Is Spec-Driven Development just a new name for waterfall?">
No. The article explicitly states this is **not** a "500-page waterfall document". It is **Agile Specification-Driven Development**. The spec is treated as a "living document", and work is broken into small "Work Packages (1–2 days each)" that function like sprints. It combines the "continuous feedback" of Agile with the "risk mitigation" of construction.
</Collapsible>

<Collapsible title="What is 'vibe coding' and why is it so bad?">
Vibe coding is an approach where you "kind of vaguely know what you want... so you just throw some code together until it feels right". It's a "prompt, paste, and hope" workflow. It's bad because it creates massive, hard-to-fix problems, including an 8-fold increase in code duplication, "overly complex" spaghetti code, and significant security time bombs.
</Collapsible>

<Collapsible title="How does a spec actually prevent AI-generated technical debt?">
AI models, when used without direction, produce "non-optimal solution[s]" that get "copy-pasted" repeatedly. This creates massive code duplication and debt. A spec prevents this by acting as a "North Star" that separates the **"stable 'what'"** (the spec) from the **"flexible 'how'"** (the AI-generated code). The AI is given clear, unambiguous instructions for a small "work package," which dramatically lowers the probability of error.
</Collapsible>

<Collapsible title="How does this workflow handle AI's security risks?">
AI-generated code is known to "unconsciously reproduc[e] known vulnerabilities". The article identifies the "Lethal Trifecta" of risk as "Sensitive Data + Untrusted Content + External Communication". The SDD loop–where you spec, build, and verify–functions as a "constant risk assessment". It systematically manages risk by:

1. **Lowering probability:** A clear spec reduces AI errors.
2. **Lowering impact:** Small "work packages" isolate any errors.
3. **Maximizing detectability:** You run "acceptance checks" to verify every slice of work.
</Collapsible>

<Collapsible title="What if I'm not technical? How can I write a spec?">
This is a common challenge caused by "unknown unknowns". The article proposes using an **"adversarial" or "critique" framework** with AI itself.

1. **Generate:** Ask one AI (e.g., Claude) to generate a full technical spec for your goal.
2. **Critique:** Ask a *different* AI (e.g., GPT-4o) to act as a "cynical, senior-level 'reviewer'" and find every flaw, risk, and missing piece in that spec.

This "LLM-as-reviewer" process helps you use AI to surface the very "unknowns" that would otherwise sink your project.
</Collapsible>

<Collapsible title="What tools are best for Spec-Driven Development?">
The spec-driven mindset pairs best with tools built for it. The article recommends **"Terminal-first, Plan-then-Execute Agents (e.g., Claude Code)"** because they are "designed for this workflow". These tools "encourage you to review a plan *before* files are changed". While Agentic IDEs (like Cursor) are powerful, they "can be a short path to vibe-coding debt if you skip the spec".
</Collapsible>

---

*By David Leer*